# -*- coding: utf-8 -*-
"""Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UJSGReJ9Eg8ISKe7YTi-xQ5foWTC3L_S
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data=pd.read_csv('TWITTER.csv',header=None)
data.columns=['ID','Sentiment','SentimentText']
data

df1=data[(data.Sentiment=='positive')]
df2=data[(data.Sentiment=='negative')]
df=[df1,df2]
final_data=pd.concat(df)
final_data

fd1=final_data.replace({'Sentiment':{'positive':'1'}})
fd2=final_data.replace({'Sentiment':{'negative':'0'}})
fd=[fd1,fd2]
final=pd.concat(fd)
final

"""SPLITTING TRAIN TEST DATA

"""

X= final_data['SentimentText']
Y=final_data['Sentiment']
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=0)

Y_train=Y_train.replace({'negative':0,'positive':1})

Y_test=Y_test.replace({'negative':0,'positive':1})

Y_train

class LogReg():
  def __init__(self, learn_rate, iter):
    self.learn_rate=learning_rate
    self.iter=iter

class Fit():
  def fit(self,X,Y):
    self.X=X
    self.Y=Y
    self.examples,self.features=X.shape
    self.weights=np.zeros(self.features) #For a logistic regression model it is necessary to intialize weight
    self.bias=0
    for i in range(self.iter):
      self.weights_updated()
    return self

class Weights(): 
  def weights(self):
    A=(1/(1+np.exp(-(self.X.dot(self.weights)+self.bias))))
    temp_values=(A-self.Y.T)
    temp_values=np.reshape(temp_values,self.examples)
    diff_of_weights=np.dot(self.X.T,temp_values)/self.features #Differentiation of weights (dw)
    diff_of_bias=np.sum(temp_values)/self.features #Differentiation of bias values (dx)
    self.weights=self.weights-(self.learn_rate*diff_of_weights)
    self.bias=self.bias-(self.learn_rate*diff_of_bias)
    return self

class Sigmoid_function():
  def sigmoid(self,X):
    Sigmoid_value=1/(1+np.exp(-(X.dot(self.weights)+self.bias)))
    Y=np.where(Sigmoid_value>0.5,1,0)
    return Y

from sklearn.model_selection import GridSearchCV,cross_val_score

from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression

from sklearn.feature_extraction.text import CountVectorizer
CV=CountVectorizer(max_features=3000)
CV.fit(X_train)

CV.get_feature_names()

X_train=CV.transform(X_train)

X_test=CV.transform(X_test)

"""ACCURACY"""

model=LogisticRegression(max_iter=2000,multi_class='ovr')
cross_val_score(model,X_train,Y_train).mean()

Classifier=LogisticRegression(max_iter=10000,random_state=0)
Classifier.fit(X_train,Y_train)

Y_pred=Classifier.predict(X_test)
Y_pred

from sklearn.metrics import confusion_matrix
confusion_matrix= confusion_matrix(Y_test,Y_pred)
print(confusion_matrix)

"""ACCURACY

"""

precision=0.87
recall=0.94

print('Accuracy of logistic regression classifier on twitter dataset: {:.2f}'.format(Classifier.score(X_test, Y_test)))
print('F1 Score is:',precision/recall)

from sklearn.metrics import classification_report
print(classification_report(Y_test,Y_pred))

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

Names=[0,1]
figure, axes = plt.subplots()
Marks = np.arange(len(Names))
plt.xticks(Marks, Names)
plt.yticks(Marks, Names)
sns.heatmap(pd.DataFrame(confusion_matrix), annot=True, cmap="RdYlBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

